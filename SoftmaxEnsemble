class SoftmaxEnsemble():
    def __init__(self):
        self.theta = None
        self.phi = None

    def softmax(self,x):
        import numpy as np
        x = np.exp(x - np.max(x))
        return x / np.sum(x)
    
    def grad_softmax(self,theta):
        import numpy as np
        S = self.softmax(theta)
        return np.diag(S.flatten()) - S @ S.T
    
    def grad(self,theta,Y,H,lam=0.0):
        import numpy as np
        D = H @ self.softmax(theta) - Y
        return 2.0 * self.grad_softmax(theta) @ np.sum(H * D.reshape(-1,1),axis=0) + lam, np.linalg.norm(D) / len(Y)
    
    def loss(self,H,Y,crit='mae'):
        import numpy as np
        if crit == 'mse':
            return np.linalg.norm(Y - H @ self.phi) ** 2 / len(Y)
        elif crit == 'rmse':
            return np.linalg.norm(Y - H @ self.phi) / len(Y)
        elif crit == 'mae':
            return np.sum(np.abs(Y - H @ self.phi)) / len(Y)
        
    def predict(self,H):
        return H @ self.phi
        
    def fit(self,Y,H,max_iter=10000,epsilon=0.01,lr=0.1,lam=0.0,lEps=0.01,epochs = 0,std = 10.0):
        import numpy as np
        thetas = [np.log(1.0 / np.mean(np.abs(H - Y.reshape(-1,1)), axis=0)),1.0 / np.mean(np.abs(H - Y.reshape(-1,1)),axis=0) 
                  ,np.log(1.0 / np.mean(np.abs(H - Y.reshape(-1,1)), axis=0)),1.0 / np.mean(np.abs(H - Y.reshape(-1,1)),axis=0) ]
        if epochs > 0:
            for epoch in range(1,epochs + 1):
                thetas.append(np.random.normal(np.zeros(H.shape[1]),std))
        losses = np.zeros(len(thetas))
        losses[0] = np.linalg.norm( H @ thetas[0] - Y ) / len(Y)
        losses[1] = np.linalg.norm( H @ thetas[1] - Y ) / len(Y)
        for epoch in range(2,epochs + 4):
            iter = 0
            diff = np.copy(epsilon) + 1.0
            theta = thetas[epoch]              
            prev_loss = np.linalg.norm( Y - H @ self.softmax(theta) ) / len(Y)
            loss = prev_loss - 1.0
            while iter < max_iter and diff > epsilon and prev_loss - loss > lEps:
                prev_loss = np.copy(loss)
                move, loss = self.grad(theta,Y,H,lam=lam)
                theta -= lr * move
                diff = np.linalg.norm(lr * move)
                iter += 1
                if prev_loss - loss <= 0:
                    print("error")
                if iter % 500 == 0:
                    print(loss)
            losses[epochs] = loss
            print(iter)
            thetas[epochs] = theta
        #if np.linalg.norm(Y - H @ self.softmax(theta)) / len(Y) < m:
        self.theta = thetas[np.argmin(losses)]
        self.phi = self.softmax(self.theta)
        return self.theta, self.phi
